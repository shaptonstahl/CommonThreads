{
    "contents" : "#' Which variables best explain membership?\n#'\n#' Identify which variables explain best why these observations/nodes\n#' are distinct from those observations.\n#' \n#' @param X data.frame of observations\n#' @param members numeric vector of rows included or logical vector with TRUE for 'members' (included rows)\n#' @param min.nonzero.certainty numeric p-value must be below 1-min.nonzero.certainty to report result for a coefficient\n#' @param classifier character singleton specifying which of \\code{probit} or \\code{forest} will be used\n#' @return list containing the following:\n#' \\tabular{lll}{\n#'   \\code{threads} \\tab \\preformatted{  } \\tab A \\code{data.frame} with columns \\code{column.name}, \n#'     \\code{effect}, and \\code{prob.not.zero}; see \\emph{Details}\\cr\n#'   \\code{X} \\tab \\tab as passed to \\code{CommonThreads}\\cr\n#'   \\code{members} \\tab \\tab as passed to \\code{CommonThreads}\\cr\n#'   \\code{min.nonzero.certainty} \\tab \\tab as passed to \\code{CommonThreads}\n#' }\n#' @details\n#' \\code{CommonThreads} uses a binary classifier (probit regression for now; random forest classifier forthcoming) with\n#' membership in \\code{members} as the dependent variable to identify which variables are best at predicting\n#' membership.\n#' \n#' A column \\code{y} of 1s and 0s is created, length equal to the number of rows of \\code{X}, with 1 for each\n#' row that is a member and 0 for each other row.  This is the dependent variable of the classifier. The columns\n#' of \\code{X} are the independent variables. Then the classifier is run and the more influential variables\n#' are identified.\n#' \n#' The returned data.frame \\code{threads} has three columns. The first, \\code{column.name}, comes from \n#' the name of the column of \\code{X}. If a column of \\code{X} was a \\code{factor} or coerced into one \n#' then \\code{column.name} will be the name of the column of \\code{X}, a period, and the value of the \n#' factor variable that was identified to be influential.  The second column gives the size of the effect \n#' for each variable but it is recommended that this be used only for comparison with other variables.\n#' \n#' The third column, \\code{prob.not.zero}, actually gives one minus the p-value associated with the \n#' coefficient. Frequentist purists will argue that this is not the probability of the alternative (non-zero)\n#' hypothesis being true; purist Bayesians will argue that this is not a proper posterior probability of anything.\n#' A thoughtful pragmatist will note that this is a good frequentist estimate for the Bayesian posterior \n#' probability that the coefficient does not have the wrong sign, assuming a flat (uninformative) prior and\n#' that there is a sufficient amount of data to have reached Asymptopia. Use with care, if at all.\n#' \n#' \\subsection{Probit Classifier}{\n#' Before the probit regression is run, each variable in \\code{X} is standardized by subtracting the column\n#' mean and dividing by the column standard deviation. This forces the units of the resulting coefficients\n#' to be \\dQuote{change in latent variable per standard deviation of x}. For linear regression this would \n#' have the same effect as standardizing the coefficients in that the coefficients would be comparable directly\n#' with each other to see which has the greatest effect. Here with probit regression it makes the size of \n#' the regression coefficient larger precisely when a change in a variable from one standard deviation below \n#' the mean to one above the mean has the larger change in the predicted probability of being a member.\n#' \n#' The returned \\code{effect} is not intended to be used directly, but can be useful in comparing the size of \n#' effects.  For example, if V1 has an effect of 2 and V1 has an effect of 1, V1 is \\dQuote{twice as influential}\n#' in predicting membership, though what that means exactly is more subtle.\n#' }\n#' \n#' \\subsection{Random Forest Classifier}{\n#' \\emph{not implemented; forthcoming}\n#' }\n#' \n#' \\subsection{Factor Variables}{\n#' If a column of \\code{X} is a \\code{factor} or can be coerced into one by \\code{dummy.data.frame(dummies)}\n#' (say, because it is a character variable) then it will be broken into a set of dummy variables, one\n#' for each value of the column.  Each of these dummy variables is tested for its influence over membership\n#' but combinations of values are not considered.\n#' }\n#' @export\n#' @seealso Use the output to describe the relationships in plain text with \\code{\\link{CommonThreadDescriptions}}.\n#' @author Stephen R. Haptonstahl \\email{srh@@haptonstahl.org}\n#' @examples\n#' data(iris)\n#' observations.selected <- sample(1:nrow(iris), 30)\n#' CommonThreads(X=iris, members=observations.selected)\n#' CommonThreads(X=iris, members=observations.selected, min.nonzero.certainty=.5)\nCommonThreads <- function(X,\n                          members,\n                          min.nonzero.certainty=.75,\n                          classifier=c(\"probit\", \"forest\")) {\n  # Guardians\n  stopifnot(is(X, \"data.frame\") && nrow(X) >= 2,\n            (is(members, \"logical\") && length(members) == nrow(X)) || \n              (is(members, \"numeric\") && is.subset(members, 1:nrow(X))),\n            min.nonzero.certainty > 0 && min.nonzero.certainty < 1\n  )\n  \n  ###  deal with default and missing values  ###\n  \n  # Will work with Z internally\n  Z <- dummy.data.frame(X, sep=\".\")  # package::dummies; NOT SURE ABOUT THIS STEP\n  Z <- scale(Z)   # makes each column mean=0, stdev=1\n  \n  # code dependent variable, taking out repeated elements\n  dep.var.name <- paste(\"cty\", paste(sample(letters, 10), collapse=\"\"), sep=\".\")  # random name to avoid colision\n  if( is(members, \"logical\") ) members <- which(members)\n  members <- unique(members)  # remove duplicate indices\n  dep.var <- rep(0, nrow(X))\n  dep.var[members] <- 1\n  Z <- data.frame(dep.var, Z)\n  names(Z)[1] <- dep.var.name\n  \n  # perform the function\n  probit.res <- glm(formula(paste(dep.var.name, \"~ .\")), \n                    data=Z, \n                    family=binomial(link=\"probit\"))\n  \n  # prepare and return the output\n  threads <- summary(probit.res)$coefficients[-1,]\n  threads <- threads[sort(abs(threads), decreasing=TRUE, index.return=TRUE)$ix,]  # sort by absolute value of coefficient\n  threads <- threads[threads[,4] < 1 - min.nonzero.certainty,]\n  threads <- data.frame(rownames(threads), threads)\n  names(threads) <- c(\"column.name\", \"effect\", \"std.err\", \"z.value\", \"p.value\")\n  threads$prob.not.zero <- 1 - threads$p.value\n  threads <- threads[,c(\"column.name\", \"effect\", \"prob.not.zero\")]\n  rownames(threads) <- NULL\n  \n  out <- list(threads=threads, \n              X=X,\n              members=members,\n              min.nonzero.certainty=min.nonzero.certainty)\n  \n  return(out)\n}\n",
    "created" : 1392594487698.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3061047598",
    "id" : "ADFCAB13",
    "lastKnownWriteTime" : 1399511361,
    "path" : "~/GitHub/CommonThreads/Rproject/dev/CommonThreads/R/CommonThreads.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}